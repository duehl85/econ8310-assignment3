{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3936dfad-73f2-4ad3-a21f-dc1c331dcdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import struct\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "#create custom class to load data\n",
    "\n",
    "class FashionMNIST(Dataset):\n",
    "    def __init__(self, images_path, labels_path):\n",
    "        self.images = self._read_images(images_path)\n",
    "        self.labels = self._read_labels(labels_path)\n",
    "\n",
    "    def _read_images(self, path):\n",
    "        with gzip.open(path, 'rb') as f:\n",
    "            magic, num, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "            data = np.frombuffer(f.read(), dtype=np.uint8).reshape(num, 1, rows, cols)\n",
    "            data = data.astype(np.float32) / 255.0\n",
    "            return torch.tensor(data, dtype=torch.float32)\n",
    "\n",
    "    def _read_labels(self, path):\n",
    "        with gzip.open(path, 'rb') as f:\n",
    "            magic, num = struct.unpack(\">II\", f.read(8))\n",
    "            data = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "            return torch.tensor(data, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "799ffa43-78dd-4c6e-8941-b752fa0c61e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths for data access\n",
    "data_path = '/Users/danieluehling/Documents/GitHub/fashion-mnist/data/fashion'\n",
    "\n",
    "train_images_path = '/Users/danieluehling/Documents/GitHub/fashion-mnist/data/fashion/train-images-idx3-ubyte.gz'\n",
    "train_labels_path = '/Users/danieluehling/Documents/GitHub/fashion-mnist/data/fashion/train-labels-idx1-ubyte.gz'\n",
    "test_images_path  = '/Users/danieluehling/Documents/GitHub/fashion-mnist/data/fashion/t10k-images-idx3-ubyte.gz'\n",
    "test_labels_path  = '/Users/danieluehling/Documents/GitHub/fashion-mnist/data/fashion/t10k-labels-idx1-ubyte.gz'\n",
    "\n",
    "# load into dataset\n",
    "train_data = FashionMNIST(train_images_path, train_labels_path)\n",
    "test_data  = FashionMNIST(test_images_path, test_labels_path)\n",
    "\n",
    "# data loaders\n",
    "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99398a2-51cd-48cc-af21-dd98d1ad3359",
   "metadata": {},
   "source": [
    "**ResNet18**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06565195-33de-4e00-8dc1-acf0711d4398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "------------------------------\n",
      "loss: 2.610945 [    0/60000]\n",
      "loss: 0.933686 [  640/60000]\n",
      "loss: 0.855690 [ 1280/60000]\n",
      "loss: 0.929078 [ 1920/60000]\n",
      "loss: 0.642682 [ 2560/60000]\n",
      "loss: 0.612818 [ 3200/60000]\n",
      "loss: 0.809693 [ 3840/60000]\n",
      "loss: 0.546354 [ 4480/60000]\n",
      "loss: 0.520474 [ 5120/60000]\n",
      "loss: 0.272110 [ 5760/60000]\n",
      "loss: 0.599447 [ 6400/60000]\n",
      "loss: 0.613149 [ 7040/60000]\n",
      "loss: 0.530319 [ 7680/60000]\n",
      "loss: 0.419030 [ 8320/60000]\n",
      "loss: 0.631364 [ 8960/60000]\n",
      "loss: 0.672996 [ 9600/60000]\n",
      "loss: 0.553733 [10240/60000]\n",
      "loss: 0.436740 [10880/60000]\n",
      "loss: 0.593849 [11520/60000]\n",
      "loss: 0.560120 [12160/60000]\n",
      "loss: 0.567501 [12800/60000]\n",
      "loss: 0.524435 [13440/60000]\n",
      "loss: 0.431685 [14080/60000]\n",
      "loss: 0.404833 [14720/60000]\n",
      "loss: 0.322221 [15360/60000]\n",
      "loss: 0.535387 [16000/60000]\n",
      "loss: 0.513838 [16640/60000]\n",
      "loss: 0.267830 [17280/60000]\n",
      "loss: 0.404571 [17920/60000]\n",
      "loss: 0.364049 [18560/60000]\n",
      "loss: 0.466163 [19200/60000]\n",
      "loss: 0.777100 [19840/60000]\n",
      "loss: 0.349647 [20480/60000]\n",
      "loss: 0.484892 [21120/60000]\n",
      "loss: 0.519165 [21760/60000]\n",
      "loss: 0.272050 [22400/60000]\n",
      "loss: 0.462821 [23040/60000]\n",
      "loss: 0.211387 [23680/60000]\n",
      "loss: 0.312960 [24320/60000]\n",
      "loss: 0.485462 [24960/60000]\n",
      "loss: 0.421069 [25600/60000]\n",
      "loss: 0.658398 [26240/60000]\n",
      "loss: 0.290359 [26880/60000]\n",
      "loss: 0.265586 [27520/60000]\n",
      "loss: 0.185117 [28160/60000]\n",
      "loss: 0.370814 [28800/60000]\n",
      "loss: 0.419093 [29440/60000]\n",
      "loss: 0.448391 [30080/60000]\n",
      "loss: 0.360486 [30720/60000]\n",
      "loss: 0.373737 [31360/60000]\n",
      "loss: 0.448678 [32000/60000]\n",
      "loss: 0.651246 [32640/60000]\n",
      "loss: 0.524774 [33280/60000]\n",
      "loss: 0.472742 [33920/60000]\n",
      "loss: 0.251438 [34560/60000]\n",
      "loss: 0.326164 [35200/60000]\n",
      "loss: 0.344669 [35840/60000]\n",
      "loss: 0.348997 [36480/60000]\n",
      "loss: 0.254870 [37120/60000]\n",
      "loss: 0.265730 [37760/60000]\n",
      "loss: 0.394032 [38400/60000]\n",
      "loss: 0.413035 [39040/60000]\n",
      "loss: 0.325327 [39680/60000]\n",
      "loss: 0.614622 [40320/60000]\n",
      "loss: 0.319479 [40960/60000]\n",
      "loss: 0.335376 [41600/60000]\n",
      "loss: 0.358137 [42240/60000]\n",
      "loss: 0.434545 [42880/60000]\n",
      "loss: 0.403294 [43520/60000]\n",
      "loss: 0.365059 [44160/60000]\n",
      "loss: 0.543371 [44800/60000]\n",
      "loss: 0.541512 [45440/60000]\n",
      "loss: 0.403663 [46080/60000]\n",
      "loss: 0.336505 [46720/60000]\n",
      "loss: 0.487508 [47360/60000]\n",
      "loss: 0.261560 [48000/60000]\n",
      "loss: 0.374197 [48640/60000]\n",
      "loss: 0.413171 [49280/60000]\n",
      "loss: 0.512448 [49920/60000]\n",
      "loss: 0.442976 [50560/60000]\n",
      "loss: 0.399322 [51200/60000]\n",
      "loss: 0.259414 [51840/60000]\n",
      "loss: 0.534783 [52480/60000]\n",
      "loss: 0.374939 [53120/60000]\n",
      "loss: 0.438146 [53760/60000]\n",
      "loss: 0.227973 [54400/60000]\n",
      "loss: 0.423068 [55040/60000]\n",
      "loss: 0.224911 [55680/60000]\n",
      "loss: 0.624318 [56320/60000]\n",
      "loss: 0.391393 [56960/60000]\n",
      "loss: 0.296410 [57600/60000]\n",
      "loss: 0.242395 [58240/60000]\n",
      "loss: 0.405958 [58880/60000]\n",
      "loss: 0.444810 [59520/60000]\n",
      "Test Accuracy: 87.7%, Avg loss: 0.331724\n",
      "\n",
      "Epoch 2\n",
      "------------------------------\n",
      "loss: 0.291501 [    0/60000]\n",
      "loss: 0.328853 [  640/60000]\n",
      "loss: 0.239221 [ 1280/60000]\n",
      "loss: 0.345303 [ 1920/60000]\n",
      "loss: 0.331073 [ 2560/60000]\n",
      "loss: 0.376581 [ 3200/60000]\n",
      "loss: 0.203866 [ 3840/60000]\n",
      "loss: 0.256222 [ 4480/60000]\n",
      "loss: 0.355087 [ 5120/60000]\n",
      "loss: 0.219433 [ 5760/60000]\n",
      "loss: 0.239407 [ 6400/60000]\n",
      "loss: 0.325720 [ 7040/60000]\n",
      "loss: 0.414024 [ 7680/60000]\n",
      "loss: 0.371767 [ 8320/60000]\n",
      "loss: 0.169744 [ 8960/60000]\n",
      "loss: 0.280639 [ 9600/60000]\n",
      "loss: 0.406976 [10240/60000]\n",
      "loss: 0.313800 [10880/60000]\n",
      "loss: 0.407487 [11520/60000]\n",
      "loss: 0.348326 [12160/60000]\n",
      "loss: 0.230487 [12800/60000]\n",
      "loss: 0.198894 [13440/60000]\n",
      "loss: 0.409073 [14080/60000]\n",
      "loss: 0.344271 [14720/60000]\n",
      "loss: 0.277707 [15360/60000]\n",
      "loss: 0.375802 [16000/60000]\n",
      "loss: 0.241087 [16640/60000]\n",
      "loss: 0.272350 [17280/60000]\n",
      "loss: 0.394584 [17920/60000]\n",
      "loss: 0.230997 [18560/60000]\n",
      "loss: 0.136116 [19200/60000]\n",
      "loss: 0.394596 [19840/60000]\n",
      "loss: 0.374734 [20480/60000]\n",
      "loss: 0.241593 [21120/60000]\n",
      "loss: 0.225624 [21760/60000]\n",
      "loss: 0.305283 [22400/60000]\n",
      "loss: 0.250940 [23040/60000]\n",
      "loss: 0.333809 [23680/60000]\n",
      "loss: 0.326731 [24320/60000]\n",
      "loss: 0.264495 [24960/60000]\n",
      "loss: 0.396163 [25600/60000]\n",
      "loss: 0.270439 [26240/60000]\n",
      "loss: 0.163940 [26880/60000]\n",
      "loss: 0.233773 [27520/60000]\n",
      "loss: 0.302995 [28160/60000]\n",
      "loss: 0.582832 [28800/60000]\n",
      "loss: 0.288883 [29440/60000]\n",
      "loss: 0.404314 [30080/60000]\n",
      "loss: 0.205715 [30720/60000]\n",
      "loss: 0.364554 [31360/60000]\n",
      "loss: 0.326618 [32000/60000]\n",
      "loss: 0.324206 [32640/60000]\n",
      "loss: 0.585185 [33280/60000]\n",
      "loss: 0.309762 [33920/60000]\n",
      "loss: 0.283352 [34560/60000]\n",
      "loss: 0.288065 [35200/60000]\n",
      "loss: 0.515113 [35840/60000]\n",
      "loss: 0.334605 [36480/60000]\n",
      "loss: 0.326758 [37120/60000]\n",
      "loss: 0.216930 [37760/60000]\n",
      "loss: 0.361270 [38400/60000]\n",
      "loss: 0.385699 [39040/60000]\n",
      "loss: 0.261936 [39680/60000]\n",
      "loss: 0.473049 [40320/60000]\n",
      "loss: 0.324583 [40960/60000]\n",
      "loss: 0.208762 [41600/60000]\n",
      "loss: 0.214083 [42240/60000]\n",
      "loss: 0.205127 [42880/60000]\n",
      "loss: 0.211715 [43520/60000]\n",
      "loss: 0.153490 [44160/60000]\n",
      "loss: 0.182207 [44800/60000]\n",
      "loss: 0.507166 [45440/60000]\n",
      "loss: 0.285499 [46080/60000]\n",
      "loss: 0.306889 [46720/60000]\n",
      "loss: 0.362675 [47360/60000]\n",
      "loss: 0.508897 [48000/60000]\n",
      "loss: 0.598787 [48640/60000]\n",
      "loss: 0.253234 [49280/60000]\n",
      "loss: 0.380483 [49920/60000]\n",
      "loss: 0.158602 [50560/60000]\n",
      "loss: 0.198969 [51200/60000]\n",
      "loss: 0.428862 [51840/60000]\n",
      "loss: 0.522668 [52480/60000]\n",
      "loss: 0.305171 [53120/60000]\n",
      "loss: 0.307105 [53760/60000]\n",
      "loss: 0.232196 [54400/60000]\n",
      "loss: 0.480082 [55040/60000]\n",
      "loss: 0.426235 [55680/60000]\n",
      "loss: 0.319499 [56320/60000]\n",
      "loss: 0.287960 [56960/60000]\n",
      "loss: 0.326762 [57600/60000]\n",
      "loss: 0.383880 [58240/60000]\n",
      "loss: 0.365244 [58880/60000]\n",
      "loss: 0.250580 [59520/60000]\n",
      "Test Accuracy: 89.3%, Avg loss: 0.302474\n",
      "\n",
      "Epoch 3\n",
      "------------------------------\n",
      "loss: 0.285640 [    0/60000]\n",
      "loss: 0.216174 [  640/60000]\n",
      "loss: 0.264349 [ 1280/60000]\n",
      "loss: 0.142266 [ 1920/60000]\n",
      "loss: 0.143315 [ 2560/60000]\n",
      "loss: 0.395003 [ 3200/60000]\n",
      "loss: 0.308264 [ 3840/60000]\n",
      "loss: 0.274828 [ 4480/60000]\n",
      "loss: 0.354204 [ 5120/60000]\n",
      "loss: 0.141321 [ 5760/60000]\n",
      "loss: 0.219992 [ 6400/60000]\n",
      "loss: 0.304831 [ 7040/60000]\n",
      "loss: 0.202868 [ 7680/60000]\n",
      "loss: 0.169701 [ 8320/60000]\n",
      "loss: 0.198872 [ 8960/60000]\n",
      "loss: 0.207698 [ 9600/60000]\n",
      "loss: 0.233955 [10240/60000]\n",
      "loss: 0.299702 [10880/60000]\n",
      "loss: 0.099181 [11520/60000]\n",
      "loss: 0.268591 [12160/60000]\n",
      "loss: 0.221981 [12800/60000]\n",
      "loss: 0.150285 [13440/60000]\n",
      "loss: 0.383249 [14080/60000]\n",
      "loss: 0.255154 [14720/60000]\n",
      "loss: 0.243202 [15360/60000]\n",
      "loss: 0.235604 [16000/60000]\n",
      "loss: 0.312703 [16640/60000]\n",
      "loss: 0.131425 [17280/60000]\n",
      "loss: 0.227913 [17920/60000]\n",
      "loss: 0.224046 [18560/60000]\n",
      "loss: 0.355815 [19200/60000]\n",
      "loss: 0.190459 [19840/60000]\n",
      "loss: 0.332107 [20480/60000]\n",
      "loss: 0.370276 [21120/60000]\n",
      "loss: 0.183983 [21760/60000]\n",
      "loss: 0.196621 [22400/60000]\n",
      "loss: 0.365266 [23040/60000]\n",
      "loss: 0.342736 [23680/60000]\n",
      "loss: 0.238172 [24320/60000]\n",
      "loss: 0.306818 [24960/60000]\n",
      "loss: 0.368902 [25600/60000]\n",
      "loss: 0.230670 [26240/60000]\n",
      "loss: 0.243800 [26880/60000]\n",
      "loss: 0.380865 [27520/60000]\n",
      "loss: 0.187486 [28160/60000]\n",
      "loss: 0.402808 [28800/60000]\n",
      "loss: 0.490033 [29440/60000]\n",
      "loss: 0.296286 [30080/60000]\n",
      "loss: 0.321131 [30720/60000]\n",
      "loss: 0.101841 [31360/60000]\n",
      "loss: 0.215409 [32000/60000]\n",
      "loss: 0.130461 [32640/60000]\n",
      "loss: 0.277477 [33280/60000]\n",
      "loss: 0.235324 [33920/60000]\n",
      "loss: 0.177278 [34560/60000]\n",
      "loss: 0.503834 [35200/60000]\n",
      "loss: 0.337217 [35840/60000]\n",
      "loss: 0.310982 [36480/60000]\n",
      "loss: 0.210961 [37120/60000]\n",
      "loss: 0.301396 [37760/60000]\n",
      "loss: 0.218499 [38400/60000]\n",
      "loss: 0.359471 [39040/60000]\n",
      "loss: 0.235667 [39680/60000]\n",
      "loss: 0.298892 [40320/60000]\n",
      "loss: 0.312071 [40960/60000]\n",
      "loss: 0.285272 [41600/60000]\n",
      "loss: 0.286210 [42240/60000]\n",
      "loss: 0.456326 [42880/60000]\n",
      "loss: 0.274934 [43520/60000]\n",
      "loss: 0.176004 [44160/60000]\n",
      "loss: 0.384942 [44800/60000]\n",
      "loss: 0.273616 [45440/60000]\n",
      "loss: 0.242822 [46080/60000]\n",
      "loss: 0.162419 [46720/60000]\n",
      "loss: 0.401204 [47360/60000]\n",
      "loss: 0.337475 [48000/60000]\n",
      "loss: 0.250519 [48640/60000]\n",
      "loss: 0.242931 [49280/60000]\n",
      "loss: 0.329966 [49920/60000]\n",
      "loss: 0.198776 [50560/60000]\n",
      "loss: 0.132350 [51200/60000]\n",
      "loss: 0.173320 [51840/60000]\n",
      "loss: 0.184520 [52480/60000]\n",
      "loss: 0.208460 [53120/60000]\n",
      "loss: 0.340545 [53760/60000]\n",
      "loss: 0.257217 [54400/60000]\n",
      "loss: 0.342958 [55040/60000]\n",
      "loss: 0.207857 [55680/60000]\n",
      "loss: 0.320975 [56320/60000]\n",
      "loss: 0.286101 [56960/60000]\n",
      "loss: 0.306310 [57600/60000]\n",
      "loss: 0.379254 [58240/60000]\n",
      "loss: 0.491251 [58880/60000]\n",
      "loss: 0.337506 [59520/60000]\n",
      "Test Accuracy: 88.1%, Avg loss: 0.326459\n",
      "\n",
      "Epoch 4\n",
      "------------------------------\n",
      "loss: 0.236742 [    0/60000]\n",
      "loss: 0.189510 [  640/60000]\n",
      "loss: 0.126848 [ 1280/60000]\n",
      "loss: 0.147991 [ 1920/60000]\n",
      "loss: 0.261329 [ 2560/60000]\n",
      "loss: 0.239220 [ 3200/60000]\n",
      "loss: 0.286621 [ 3840/60000]\n",
      "loss: 0.264851 [ 4480/60000]\n",
      "loss: 0.205530 [ 5120/60000]\n",
      "loss: 0.103963 [ 5760/60000]\n",
      "loss: 0.214963 [ 6400/60000]\n",
      "loss: 0.236389 [ 7040/60000]\n",
      "loss: 0.209367 [ 7680/60000]\n",
      "loss: 0.309508 [ 8320/60000]\n",
      "loss: 0.198481 [ 8960/60000]\n",
      "loss: 0.195764 [ 9600/60000]\n",
      "loss: 0.142958 [10240/60000]\n",
      "loss: 0.228332 [10880/60000]\n",
      "loss: 0.193428 [11520/60000]\n",
      "loss: 0.262177 [12160/60000]\n",
      "loss: 0.240679 [12800/60000]\n",
      "loss: 0.246475 [13440/60000]\n",
      "loss: 0.109597 [14080/60000]\n",
      "loss: 0.320513 [14720/60000]\n",
      "loss: 0.135230 [15360/60000]\n",
      "loss: 0.280808 [16000/60000]\n",
      "loss: 0.228246 [16640/60000]\n",
      "loss: 0.294823 [17280/60000]\n",
      "loss: 0.331609 [17920/60000]\n",
      "loss: 0.255318 [18560/60000]\n",
      "loss: 0.169236 [19200/60000]\n",
      "loss: 0.251002 [19840/60000]\n",
      "loss: 0.219382 [20480/60000]\n",
      "loss: 0.176061 [21120/60000]\n",
      "loss: 0.247364 [21760/60000]\n",
      "loss: 0.098865 [22400/60000]\n",
      "loss: 0.179248 [23040/60000]\n",
      "loss: 0.468705 [23680/60000]\n",
      "loss: 0.203363 [24320/60000]\n",
      "loss: 0.334431 [24960/60000]\n",
      "loss: 0.328363 [25600/60000]\n",
      "loss: 0.302862 [26240/60000]\n",
      "loss: 0.205120 [26880/60000]\n",
      "loss: 0.243962 [27520/60000]\n",
      "loss: 0.287198 [28160/60000]\n",
      "loss: 0.213164 [28800/60000]\n",
      "loss: 0.165216 [29440/60000]\n",
      "loss: 0.360784 [30080/60000]\n",
      "loss: 0.199940 [30720/60000]\n",
      "loss: 0.341734 [31360/60000]\n",
      "loss: 0.108282 [32000/60000]\n",
      "loss: 0.177169 [32640/60000]\n",
      "loss: 0.256865 [33280/60000]\n",
      "loss: 0.254331 [33920/60000]\n",
      "loss: 0.193928 [34560/60000]\n",
      "loss: 0.163472 [35200/60000]\n",
      "loss: 0.215312 [35840/60000]\n",
      "loss: 0.389875 [36480/60000]\n",
      "loss: 0.147762 [37120/60000]\n",
      "loss: 0.171616 [37760/60000]\n",
      "loss: 0.299466 [38400/60000]\n",
      "loss: 0.175132 [39040/60000]\n",
      "loss: 0.298870 [39680/60000]\n",
      "loss: 0.359615 [40320/60000]\n",
      "loss: 0.148087 [40960/60000]\n",
      "loss: 0.152728 [41600/60000]\n",
      "loss: 0.308709 [42240/60000]\n",
      "loss: 0.251250 [42880/60000]\n",
      "loss: 0.246158 [43520/60000]\n",
      "loss: 0.342375 [44160/60000]\n",
      "loss: 0.191589 [44800/60000]\n",
      "loss: 0.136314 [45440/60000]\n",
      "loss: 0.208871 [46080/60000]\n",
      "loss: 0.241765 [46720/60000]\n",
      "loss: 0.171019 [47360/60000]\n",
      "loss: 0.408251 [48000/60000]\n",
      "loss: 0.221051 [48640/60000]\n",
      "loss: 0.393372 [49280/60000]\n",
      "loss: 0.164996 [49920/60000]\n",
      "loss: 0.197239 [50560/60000]\n",
      "loss: 0.270024 [51200/60000]\n",
      "loss: 0.223677 [51840/60000]\n",
      "loss: 0.206001 [52480/60000]\n",
      "loss: 0.261285 [53120/60000]\n",
      "loss: 0.237030 [53760/60000]\n",
      "loss: 0.187074 [54400/60000]\n",
      "loss: 0.277982 [55040/60000]\n",
      "loss: 0.230590 [55680/60000]\n",
      "loss: 0.128861 [56320/60000]\n",
      "loss: 0.208225 [56960/60000]\n",
      "loss: 0.290283 [57600/60000]\n",
      "loss: 0.207640 [58240/60000]\n",
      "loss: 0.239817 [58880/60000]\n",
      "loss: 0.262294 [59520/60000]\n",
      "Test Accuracy: 89.7%, Avg loss: 0.285921\n",
      "\n",
      "Epoch 5\n",
      "------------------------------\n",
      "loss: 0.204935 [    0/60000]\n",
      "loss: 0.168536 [  640/60000]\n",
      "loss: 0.265391 [ 1280/60000]\n",
      "loss: 0.238841 [ 1920/60000]\n",
      "loss: 0.111546 [ 2560/60000]\n",
      "loss: 0.189525 [ 3200/60000]\n",
      "loss: 0.146892 [ 3840/60000]\n",
      "loss: 0.106880 [ 4480/60000]\n",
      "loss: 0.253444 [ 5120/60000]\n",
      "loss: 0.124908 [ 5760/60000]\n",
      "loss: 0.316355 [ 6400/60000]\n",
      "loss: 0.176492 [ 7040/60000]\n",
      "loss: 0.406995 [ 7680/60000]\n",
      "loss: 0.239560 [ 8320/60000]\n",
      "loss: 0.310766 [ 8960/60000]\n",
      "loss: 0.154356 [ 9600/60000]\n",
      "loss: 0.305921 [10240/60000]\n",
      "loss: 0.236858 [10880/60000]\n",
      "loss: 0.153366 [11520/60000]\n",
      "loss: 0.168733 [12160/60000]\n",
      "loss: 0.290295 [12800/60000]\n",
      "loss: 0.267834 [13440/60000]\n",
      "loss: 0.380814 [14080/60000]\n",
      "loss: 0.272861 [14720/60000]\n",
      "loss: 0.210980 [15360/60000]\n",
      "loss: 0.202255 [16000/60000]\n",
      "loss: 0.206855 [16640/60000]\n",
      "loss: 0.302440 [17280/60000]\n",
      "loss: 0.260244 [17920/60000]\n",
      "loss: 0.332564 [18560/60000]\n",
      "loss: 0.263568 [19200/60000]\n",
      "loss: 0.213909 [19840/60000]\n",
      "loss: 0.137918 [20480/60000]\n",
      "loss: 0.349278 [21120/60000]\n",
      "loss: 0.067142 [21760/60000]\n",
      "loss: 0.269092 [22400/60000]\n",
      "loss: 0.265000 [23040/60000]\n",
      "loss: 0.232176 [23680/60000]\n",
      "loss: 0.212052 [24320/60000]\n",
      "loss: 0.233715 [24960/60000]\n",
      "loss: 0.173486 [25600/60000]\n",
      "loss: 0.216973 [26240/60000]\n",
      "loss: 0.244250 [26880/60000]\n",
      "loss: 0.128017 [27520/60000]\n",
      "loss: 0.124483 [28160/60000]\n",
      "loss: 0.257868 [28800/60000]\n",
      "loss: 0.472411 [29440/60000]\n",
      "loss: 0.220486 [30080/60000]\n",
      "loss: 0.215773 [30720/60000]\n",
      "loss: 0.203322 [31360/60000]\n",
      "loss: 0.205802 [32000/60000]\n",
      "loss: 0.411753 [32640/60000]\n",
      "loss: 0.184921 [33280/60000]\n",
      "loss: 0.334505 [33920/60000]\n",
      "loss: 0.163194 [34560/60000]\n",
      "loss: 0.189587 [35200/60000]\n",
      "loss: 0.237330 [35840/60000]\n",
      "loss: 0.121178 [36480/60000]\n",
      "loss: 0.311324 [37120/60000]\n",
      "loss: 0.143761 [37760/60000]\n",
      "loss: 0.148572 [38400/60000]\n",
      "loss: 0.270661 [39040/60000]\n",
      "loss: 0.119498 [39680/60000]\n",
      "loss: 0.266297 [40320/60000]\n",
      "loss: 0.198780 [40960/60000]\n",
      "loss: 0.172721 [41600/60000]\n",
      "loss: 0.331985 [42240/60000]\n",
      "loss: 0.192501 [42880/60000]\n",
      "loss: 0.249284 [43520/60000]\n",
      "loss: 0.158875 [44160/60000]\n",
      "loss: 0.352116 [44800/60000]\n",
      "loss: 0.166273 [45440/60000]\n",
      "loss: 0.336182 [46080/60000]\n",
      "loss: 0.290244 [46720/60000]\n",
      "loss: 0.129889 [47360/60000]\n",
      "loss: 0.182021 [48000/60000]\n",
      "loss: 0.224802 [48640/60000]\n",
      "loss: 0.199750 [49280/60000]\n",
      "loss: 0.246205 [49920/60000]\n",
      "loss: 0.186508 [50560/60000]\n",
      "loss: 0.182467 [51200/60000]\n",
      "loss: 0.226746 [51840/60000]\n",
      "loss: 0.212004 [52480/60000]\n",
      "loss: 0.247126 [53120/60000]\n",
      "loss: 0.267433 [53760/60000]\n",
      "loss: 0.195929 [54400/60000]\n",
      "loss: 0.130444 [55040/60000]\n",
      "loss: 0.162560 [55680/60000]\n",
      "loss: 0.144816 [56320/60000]\n",
      "loss: 0.252561 [56960/60000]\n",
      "loss: 0.292255 [57600/60000]\n",
      "loss: 0.246254 [58240/60000]\n",
      "loss: 0.202108 [58880/60000]\n",
      "loss: 0.265794 [59520/60000]\n",
      "Test Accuracy: 89.4%, Avg loss: 0.293969\n",
      "\n",
      "Epoch 6\n",
      "------------------------------\n",
      "loss: 0.235760 [    0/60000]\n",
      "loss: 0.137297 [  640/60000]\n",
      "loss: 0.272468 [ 1280/60000]\n",
      "loss: 0.209317 [ 1920/60000]\n",
      "loss: 0.099971 [ 2560/60000]\n",
      "loss: 0.125812 [ 3200/60000]\n",
      "loss: 0.168433 [ 3840/60000]\n",
      "loss: 0.252557 [ 4480/60000]\n",
      "loss: 0.210687 [ 5120/60000]\n",
      "loss: 0.260799 [ 5760/60000]\n",
      "loss: 0.283120 [ 6400/60000]\n",
      "loss: 0.133538 [ 7040/60000]\n",
      "loss: 0.201431 [ 7680/60000]\n",
      "loss: 0.149608 [ 8320/60000]\n",
      "loss: 0.282468 [ 8960/60000]\n",
      "loss: 0.325594 [ 9600/60000]\n",
      "loss: 0.224272 [10240/60000]\n",
      "loss: 0.204968 [10880/60000]\n",
      "loss: 0.232102 [11520/60000]\n",
      "loss: 0.238547 [12160/60000]\n",
      "loss: 0.241330 [12800/60000]\n",
      "loss: 0.306169 [13440/60000]\n",
      "loss: 0.135161 [14080/60000]\n",
      "loss: 0.169007 [14720/60000]\n",
      "loss: 0.173885 [15360/60000]\n",
      "loss: 0.218866 [16000/60000]\n",
      "loss: 0.140980 [16640/60000]\n",
      "loss: 0.264810 [17280/60000]\n",
      "loss: 0.230948 [17920/60000]\n",
      "loss: 0.278029 [18560/60000]\n",
      "loss: 0.144260 [19200/60000]\n",
      "loss: 0.163403 [19840/60000]\n",
      "loss: 0.275615 [20480/60000]\n",
      "loss: 0.132302 [21120/60000]\n",
      "loss: 0.204866 [21760/60000]\n",
      "loss: 0.090661 [22400/60000]\n",
      "loss: 0.153656 [23040/60000]\n",
      "loss: 0.288047 [23680/60000]\n",
      "loss: 0.213710 [24320/60000]\n",
      "loss: 0.291331 [24960/60000]\n",
      "loss: 0.144271 [25600/60000]\n",
      "loss: 0.209093 [26240/60000]\n",
      "loss: 0.125465 [26880/60000]\n",
      "loss: 0.323247 [27520/60000]\n",
      "loss: 0.248232 [28160/60000]\n",
      "loss: 0.119128 [28800/60000]\n",
      "loss: 0.160118 [29440/60000]\n",
      "loss: 0.227265 [30080/60000]\n",
      "loss: 0.318726 [30720/60000]\n",
      "loss: 0.333580 [31360/60000]\n",
      "loss: 0.184525 [32000/60000]\n",
      "loss: 0.171240 [32640/60000]\n",
      "loss: 0.133399 [33280/60000]\n",
      "loss: 0.270990 [33920/60000]\n",
      "loss: 0.224634 [34560/60000]\n",
      "loss: 0.352352 [35200/60000]\n",
      "loss: 0.169979 [35840/60000]\n",
      "loss: 0.208217 [36480/60000]\n",
      "loss: 0.247150 [37120/60000]\n",
      "loss: 0.160725 [37760/60000]\n",
      "loss: 0.153495 [38400/60000]\n",
      "loss: 0.117863 [39040/60000]\n",
      "loss: 0.115298 [39680/60000]\n",
      "loss: 0.263575 [40320/60000]\n",
      "loss: 0.154691 [40960/60000]\n",
      "loss: 0.155125 [41600/60000]\n",
      "loss: 0.184577 [42240/60000]\n",
      "loss: 0.075742 [42880/60000]\n",
      "loss: 0.128244 [43520/60000]\n",
      "loss: 0.246638 [44160/60000]\n",
      "loss: 0.118483 [44800/60000]\n",
      "loss: 0.128877 [45440/60000]\n",
      "loss: 0.085970 [46080/60000]\n",
      "loss: 0.130374 [46720/60000]\n",
      "loss: 0.106922 [47360/60000]\n",
      "loss: 0.233956 [48000/60000]\n",
      "loss: 0.231184 [48640/60000]\n",
      "loss: 0.138190 [49280/60000]\n",
      "loss: 0.193341 [49920/60000]\n",
      "loss: 0.249094 [50560/60000]\n",
      "loss: 0.318998 [51200/60000]\n",
      "loss: 0.197265 [51840/60000]\n",
      "loss: 0.115761 [52480/60000]\n",
      "loss: 0.091008 [53120/60000]\n",
      "loss: 0.200039 [53760/60000]\n",
      "loss: 0.135077 [54400/60000]\n",
      "loss: 0.185844 [55040/60000]\n",
      "loss: 0.113778 [55680/60000]\n",
      "loss: 0.145903 [56320/60000]\n",
      "loss: 0.174621 [56960/60000]\n",
      "loss: 0.240836 [57600/60000]\n",
      "loss: 0.326885 [58240/60000]\n",
      "loss: 0.171344 [58880/60000]\n",
      "loss: 0.071417 [59520/60000]\n",
      "Test Accuracy: 90.7%, Avg loss: 0.257953\n",
      "\n",
      "Epoch 7\n",
      "------------------------------\n",
      "loss: 0.246749 [    0/60000]\n",
      "loss: 0.199957 [  640/60000]\n",
      "loss: 0.296531 [ 1280/60000]\n",
      "loss: 0.080964 [ 1920/60000]\n",
      "loss: 0.124048 [ 2560/60000]\n",
      "loss: 0.146508 [ 3200/60000]\n",
      "loss: 0.119803 [ 3840/60000]\n",
      "loss: 0.206268 [ 4480/60000]\n",
      "loss: 0.104365 [ 5120/60000]\n",
      "loss: 0.100534 [ 5760/60000]\n",
      "loss: 0.244554 [ 6400/60000]\n",
      "loss: 0.145309 [ 7040/60000]\n",
      "loss: 0.167892 [ 7680/60000]\n",
      "loss: 0.162470 [ 8320/60000]\n",
      "loss: 0.191402 [ 8960/60000]\n",
      "loss: 0.092851 [ 9600/60000]\n",
      "loss: 0.184367 [10240/60000]\n",
      "loss: 0.152673 [10880/60000]\n",
      "loss: 0.224882 [11520/60000]\n",
      "loss: 0.160508 [12160/60000]\n",
      "loss: 0.184485 [12800/60000]\n",
      "loss: 0.140652 [13440/60000]\n",
      "loss: 0.125987 [14080/60000]\n",
      "loss: 0.203383 [14720/60000]\n",
      "loss: 0.106039 [15360/60000]\n",
      "loss: 0.086882 [16000/60000]\n",
      "loss: 0.086199 [16640/60000]\n",
      "loss: 0.164838 [17280/60000]\n",
      "loss: 0.159486 [17920/60000]\n",
      "loss: 0.187815 [18560/60000]\n",
      "loss: 0.216173 [19200/60000]\n",
      "loss: 0.111986 [19840/60000]\n",
      "loss: 0.180845 [20480/60000]\n",
      "loss: 0.197806 [21120/60000]\n",
      "loss: 0.131573 [21760/60000]\n",
      "loss: 0.307049 [22400/60000]\n",
      "loss: 0.165889 [23040/60000]\n",
      "loss: 0.131261 [23680/60000]\n",
      "loss: 0.101544 [24320/60000]\n",
      "loss: 0.044773 [24960/60000]\n",
      "loss: 0.172798 [25600/60000]\n",
      "loss: 0.411484 [26240/60000]\n",
      "loss: 0.248454 [26880/60000]\n",
      "loss: 0.131699 [27520/60000]\n",
      "loss: 0.186428 [28160/60000]\n",
      "loss: 0.254153 [28800/60000]\n",
      "loss: 0.095757 [29440/60000]\n",
      "loss: 0.235684 [30080/60000]\n",
      "loss: 0.243791 [30720/60000]\n",
      "loss: 0.189908 [31360/60000]\n",
      "loss: 0.300371 [32000/60000]\n",
      "loss: 0.143123 [32640/60000]\n",
      "loss: 0.294349 [33280/60000]\n",
      "loss: 0.191741 [33920/60000]\n",
      "loss: 0.149603 [34560/60000]\n",
      "loss: 0.268932 [35200/60000]\n",
      "loss: 0.160420 [35840/60000]\n",
      "loss: 0.357676 [36480/60000]\n",
      "loss: 0.091447 [37120/60000]\n",
      "loss: 0.266977 [37760/60000]\n",
      "loss: 0.345668 [38400/60000]\n",
      "loss: 0.121878 [39040/60000]\n",
      "loss: 0.374577 [39680/60000]\n",
      "loss: 0.137420 [40320/60000]\n",
      "loss: 0.137082 [40960/60000]\n",
      "loss: 0.104374 [41600/60000]\n",
      "loss: 0.238877 [42240/60000]\n",
      "loss: 0.082436 [42880/60000]\n",
      "loss: 0.141441 [43520/60000]\n",
      "loss: 0.230420 [44160/60000]\n",
      "loss: 0.148359 [44800/60000]\n",
      "loss: 0.246926 [45440/60000]\n",
      "loss: 0.157524 [46080/60000]\n",
      "loss: 0.130805 [46720/60000]\n",
      "loss: 0.139749 [47360/60000]\n",
      "loss: 0.106457 [48000/60000]\n",
      "loss: 0.134756 [48640/60000]\n",
      "loss: 0.269536 [49280/60000]\n",
      "loss: 0.228445 [49920/60000]\n",
      "loss: 0.165409 [50560/60000]\n",
      "loss: 0.165029 [51200/60000]\n",
      "loss: 0.044915 [51840/60000]\n",
      "loss: 0.175405 [52480/60000]\n",
      "loss: 0.125446 [53120/60000]\n",
      "loss: 0.204211 [53760/60000]\n",
      "loss: 0.190165 [54400/60000]\n",
      "loss: 0.206796 [55040/60000]\n",
      "loss: 0.178263 [55680/60000]\n",
      "loss: 0.159326 [56320/60000]\n",
      "loss: 0.313825 [56960/60000]\n",
      "loss: 0.158603 [57600/60000]\n",
      "loss: 0.281024 [58240/60000]\n",
      "loss: 0.196497 [58880/60000]\n",
      "loss: 0.157418 [59520/60000]\n",
      "Test Accuracy: 91.1%, Avg loss: 0.249760\n",
      "\n",
      "Epoch 8\n",
      "------------------------------\n",
      "loss: 0.255726 [    0/60000]\n",
      "loss: 0.128337 [  640/60000]\n",
      "loss: 0.140341 [ 1280/60000]\n",
      "loss: 0.124447 [ 1920/60000]\n",
      "loss: 0.234832 [ 2560/60000]\n",
      "loss: 0.405737 [ 3200/60000]\n",
      "loss: 0.214573 [ 3840/60000]\n",
      "loss: 0.183601 [ 4480/60000]\n",
      "loss: 0.260105 [ 5120/60000]\n",
      "loss: 0.151698 [ 5760/60000]\n",
      "loss: 0.242840 [ 6400/60000]\n",
      "loss: 0.119410 [ 7040/60000]\n",
      "loss: 0.173576 [ 7680/60000]\n",
      "loss: 0.081381 [ 8320/60000]\n",
      "loss: 0.233819 [ 8960/60000]\n",
      "loss: 0.161449 [ 9600/60000]\n",
      "loss: 0.046193 [10240/60000]\n",
      "loss: 0.273005 [10880/60000]\n",
      "loss: 0.100591 [11520/60000]\n",
      "loss: 0.130960 [12160/60000]\n",
      "loss: 0.178468 [12800/60000]\n",
      "loss: 0.240395 [13440/60000]\n",
      "loss: 0.073921 [14080/60000]\n",
      "loss: 0.190170 [14720/60000]\n",
      "loss: 0.138153 [15360/60000]\n",
      "loss: 0.251174 [16000/60000]\n",
      "loss: 0.153025 [16640/60000]\n",
      "loss: 0.135726 [17280/60000]\n",
      "loss: 0.102340 [17920/60000]\n",
      "loss: 0.217652 [18560/60000]\n",
      "loss: 0.185133 [19200/60000]\n",
      "loss: 0.239508 [19840/60000]\n",
      "loss: 0.112780 [20480/60000]\n",
      "loss: 0.089460 [21120/60000]\n",
      "loss: 0.211695 [21760/60000]\n",
      "loss: 0.116167 [22400/60000]\n",
      "loss: 0.133972 [23040/60000]\n",
      "loss: 0.221366 [23680/60000]\n",
      "loss: 0.123862 [24320/60000]\n",
      "loss: 0.161219 [24960/60000]\n",
      "loss: 0.212213 [25600/60000]\n",
      "loss: 0.201922 [26240/60000]\n",
      "loss: 0.087078 [26880/60000]\n",
      "loss: 0.249464 [27520/60000]\n",
      "loss: 0.163232 [28160/60000]\n",
      "loss: 0.129626 [28800/60000]\n",
      "loss: 0.175516 [29440/60000]\n",
      "loss: 0.157756 [30080/60000]\n",
      "loss: 0.091069 [30720/60000]\n",
      "loss: 0.145698 [31360/60000]\n",
      "loss: 0.103174 [32000/60000]\n",
      "loss: 0.192562 [32640/60000]\n",
      "loss: 0.068056 [33280/60000]\n",
      "loss: 0.100175 [33920/60000]\n",
      "loss: 0.149707 [34560/60000]\n",
      "loss: 0.121260 [35200/60000]\n",
      "loss: 0.121042 [35840/60000]\n",
      "loss: 0.261157 [36480/60000]\n",
      "loss: 0.186626 [37120/60000]\n",
      "loss: 0.047874 [37760/60000]\n",
      "loss: 0.092771 [38400/60000]\n",
      "loss: 0.124946 [39040/60000]\n",
      "loss: 0.172821 [39680/60000]\n",
      "loss: 0.161689 [40320/60000]\n",
      "loss: 0.080385 [40960/60000]\n",
      "loss: 0.125265 [41600/60000]\n",
      "loss: 0.226123 [42240/60000]\n",
      "loss: 0.142562 [42880/60000]\n",
      "loss: 0.155880 [43520/60000]\n",
      "loss: 0.325321 [44160/60000]\n",
      "loss: 0.107473 [44800/60000]\n",
      "loss: 0.105826 [45440/60000]\n",
      "loss: 0.244207 [46080/60000]\n",
      "loss: 0.245136 [46720/60000]\n",
      "loss: 0.226986 [47360/60000]\n",
      "loss: 0.273408 [48000/60000]\n",
      "loss: 0.095004 [48640/60000]\n",
      "loss: 0.093876 [49280/60000]\n",
      "loss: 0.221819 [49920/60000]\n",
      "loss: 0.190558 [50560/60000]\n",
      "loss: 0.080410 [51200/60000]\n",
      "loss: 0.254945 [51840/60000]\n",
      "loss: 0.237854 [52480/60000]\n",
      "loss: 0.160071 [53120/60000]\n",
      "loss: 0.104520 [53760/60000]\n",
      "loss: 0.125324 [54400/60000]\n",
      "loss: 0.150222 [55040/60000]\n",
      "loss: 0.160722 [55680/60000]\n",
      "loss: 0.119245 [56320/60000]\n",
      "loss: 0.199090 [56960/60000]\n",
      "loss: 0.206490 [57600/60000]\n",
      "loss: 0.118580 [58240/60000]\n",
      "loss: 0.098582 [58880/60000]\n",
      "loss: 0.113613 [59520/60000]\n",
      "Test Accuracy: 90.8%, Avg loss: 0.259527\n",
      "\n",
      "Epoch 9\n",
      "------------------------------\n",
      "loss: 0.124667 [    0/60000]\n",
      "loss: 0.140553 [  640/60000]\n",
      "loss: 0.092918 [ 1280/60000]\n",
      "loss: 0.090877 [ 1920/60000]\n",
      "loss: 0.119889 [ 2560/60000]\n",
      "loss: 0.109042 [ 3200/60000]\n",
      "loss: 0.101378 [ 3840/60000]\n",
      "loss: 0.155031 [ 4480/60000]\n",
      "loss: 0.165044 [ 5120/60000]\n",
      "loss: 0.183439 [ 5760/60000]\n",
      "loss: 0.119066 [ 6400/60000]\n",
      "loss: 0.178138 [ 7040/60000]\n",
      "loss: 0.084026 [ 7680/60000]\n",
      "loss: 0.247115 [ 8320/60000]\n",
      "loss: 0.130210 [ 8960/60000]\n",
      "loss: 0.193209 [ 9600/60000]\n",
      "loss: 0.229084 [10240/60000]\n",
      "loss: 0.145226 [10880/60000]\n",
      "loss: 0.188790 [11520/60000]\n",
      "loss: 0.157603 [12160/60000]\n",
      "loss: 0.098991 [12800/60000]\n",
      "loss: 0.106734 [13440/60000]\n",
      "loss: 0.091653 [14080/60000]\n",
      "loss: 0.257274 [14720/60000]\n",
      "loss: 0.258155 [15360/60000]\n",
      "loss: 0.251723 [16000/60000]\n",
      "loss: 0.122834 [16640/60000]\n",
      "loss: 0.195184 [17280/60000]\n",
      "loss: 0.298832 [17920/60000]\n",
      "loss: 0.158659 [18560/60000]\n",
      "loss: 0.075152 [19200/60000]\n",
      "loss: 0.052368 [19840/60000]\n",
      "loss: 0.162035 [20480/60000]\n",
      "loss: 0.199744 [21120/60000]\n",
      "loss: 0.101706 [21760/60000]\n",
      "loss: 0.078947 [22400/60000]\n",
      "loss: 0.188150 [23040/60000]\n",
      "loss: 0.121526 [23680/60000]\n",
      "loss: 0.119951 [24320/60000]\n",
      "loss: 0.226129 [24960/60000]\n",
      "loss: 0.179034 [25600/60000]\n",
      "loss: 0.169780 [26240/60000]\n",
      "loss: 0.065651 [26880/60000]\n",
      "loss: 0.190321 [27520/60000]\n",
      "loss: 0.129561 [28160/60000]\n",
      "loss: 0.179592 [28800/60000]\n",
      "loss: 0.249948 [29440/60000]\n",
      "loss: 0.239071 [30080/60000]\n",
      "loss: 0.076805 [30720/60000]\n",
      "loss: 0.073264 [31360/60000]\n",
      "loss: 0.143154 [32000/60000]\n",
      "loss: 0.164797 [32640/60000]\n",
      "loss: 0.163338 [33280/60000]\n",
      "loss: 0.231330 [33920/60000]\n",
      "loss: 0.230051 [34560/60000]\n",
      "loss: 0.149843 [35200/60000]\n",
      "loss: 0.147022 [35840/60000]\n",
      "loss: 0.234887 [36480/60000]\n",
      "loss: 0.146049 [37120/60000]\n",
      "loss: 0.197989 [37760/60000]\n",
      "loss: 0.136411 [38400/60000]\n",
      "loss: 0.045080 [39040/60000]\n",
      "loss: 0.147952 [39680/60000]\n",
      "loss: 0.400817 [40320/60000]\n",
      "loss: 0.234782 [40960/60000]\n",
      "loss: 0.274159 [41600/60000]\n",
      "loss: 0.149315 [42240/60000]\n",
      "loss: 0.108926 [42880/60000]\n",
      "loss: 0.133747 [43520/60000]\n",
      "loss: 0.113763 [44160/60000]\n",
      "loss: 0.151056 [44800/60000]\n",
      "loss: 0.159412 [45440/60000]\n",
      "loss: 0.076222 [46080/60000]\n",
      "loss: 0.081305 [46720/60000]\n",
      "loss: 0.259071 [47360/60000]\n",
      "loss: 0.157854 [48000/60000]\n",
      "loss: 0.080413 [48640/60000]\n",
      "loss: 0.253646 [49280/60000]\n",
      "loss: 0.194297 [49920/60000]\n",
      "loss: 0.173019 [50560/60000]\n",
      "loss: 0.092253 [51200/60000]\n",
      "loss: 0.055760 [51840/60000]\n",
      "loss: 0.163586 [52480/60000]\n",
      "loss: 0.241606 [53120/60000]\n",
      "loss: 0.205771 [53760/60000]\n",
      "loss: 0.130668 [54400/60000]\n",
      "loss: 0.206651 [55040/60000]\n",
      "loss: 0.082210 [55680/60000]\n",
      "loss: 0.166556 [56320/60000]\n",
      "loss: 0.056503 [56960/60000]\n",
      "loss: 0.102847 [57600/60000]\n",
      "loss: 0.276476 [58240/60000]\n",
      "loss: 0.114614 [58880/60000]\n",
      "loss: 0.078823 [59520/60000]\n",
      "Test Accuracy: 91.3%, Avg loss: 0.255509\n",
      "\n",
      "Epoch 10\n",
      "------------------------------\n",
      "loss: 0.140968 [    0/60000]\n",
      "loss: 0.106296 [  640/60000]\n",
      "loss: 0.137706 [ 1280/60000]\n",
      "loss: 0.149395 [ 1920/60000]\n",
      "loss: 0.254017 [ 2560/60000]\n",
      "loss: 0.160059 [ 3200/60000]\n",
      "loss: 0.276080 [ 3840/60000]\n",
      "loss: 0.213271 [ 4480/60000]\n",
      "loss: 0.164857 [ 5120/60000]\n",
      "loss: 0.195300 [ 5760/60000]\n",
      "loss: 0.171076 [ 6400/60000]\n",
      "loss: 0.119329 [ 7040/60000]\n",
      "loss: 0.090447 [ 7680/60000]\n",
      "loss: 0.070338 [ 8320/60000]\n",
      "loss: 0.075726 [ 8960/60000]\n",
      "loss: 0.112045 [ 9600/60000]\n",
      "loss: 0.301962 [10240/60000]\n",
      "loss: 0.104027 [10880/60000]\n",
      "loss: 0.300277 [11520/60000]\n",
      "loss: 0.112948 [12160/60000]\n",
      "loss: 0.110708 [12800/60000]\n",
      "loss: 0.193358 [13440/60000]\n",
      "loss: 0.124083 [14080/60000]\n",
      "loss: 0.172155 [14720/60000]\n",
      "loss: 0.091647 [15360/60000]\n",
      "loss: 0.140820 [16000/60000]\n",
      "loss: 0.226486 [16640/60000]\n",
      "loss: 0.048704 [17280/60000]\n",
      "loss: 0.067773 [17920/60000]\n",
      "loss: 0.171453 [18560/60000]\n",
      "loss: 0.159810 [19200/60000]\n",
      "loss: 0.075450 [19840/60000]\n",
      "loss: 0.116158 [20480/60000]\n",
      "loss: 0.178331 [21120/60000]\n",
      "loss: 0.034037 [21760/60000]\n",
      "loss: 0.166329 [22400/60000]\n",
      "loss: 0.230346 [23040/60000]\n",
      "loss: 0.071772 [23680/60000]\n",
      "loss: 0.103212 [24320/60000]\n",
      "loss: 0.161847 [24960/60000]\n",
      "loss: 0.116614 [25600/60000]\n",
      "loss: 0.132267 [26240/60000]\n",
      "loss: 0.202084 [26880/60000]\n",
      "loss: 0.095222 [27520/60000]\n",
      "loss: 0.081496 [28160/60000]\n",
      "loss: 0.020454 [28800/60000]\n",
      "loss: 0.127966 [29440/60000]\n",
      "loss: 0.046279 [30080/60000]\n",
      "loss: 0.251129 [30720/60000]\n",
      "loss: 0.160275 [31360/60000]\n",
      "loss: 0.131288 [32000/60000]\n",
      "loss: 0.099220 [32640/60000]\n",
      "loss: 0.186636 [33280/60000]\n",
      "loss: 0.078649 [33920/60000]\n",
      "loss: 0.428369 [34560/60000]\n",
      "loss: 0.109641 [35200/60000]\n",
      "loss: 0.208534 [35840/60000]\n",
      "loss: 0.085313 [36480/60000]\n",
      "loss: 0.074518 [37120/60000]\n",
      "loss: 0.123049 [37760/60000]\n",
      "loss: 0.188861 [38400/60000]\n",
      "loss: 0.070406 [39040/60000]\n",
      "loss: 0.137448 [39680/60000]\n",
      "loss: 0.293552 [40320/60000]\n",
      "loss: 0.070469 [40960/60000]\n",
      "loss: 0.211610 [41600/60000]\n",
      "loss: 0.041260 [42240/60000]\n",
      "loss: 0.167893 [42880/60000]\n",
      "loss: 0.127326 [43520/60000]\n",
      "loss: 0.109489 [44160/60000]\n",
      "loss: 0.208951 [44800/60000]\n",
      "loss: 0.156920 [45440/60000]\n",
      "loss: 0.098278 [46080/60000]\n",
      "loss: 0.105494 [46720/60000]\n",
      "loss: 0.106565 [47360/60000]\n",
      "loss: 0.181763 [48000/60000]\n",
      "loss: 0.227081 [48640/60000]\n",
      "loss: 0.115131 [49280/60000]\n",
      "loss: 0.126451 [49920/60000]\n",
      "loss: 0.155597 [50560/60000]\n",
      "loss: 0.130253 [51200/60000]\n",
      "loss: 0.367073 [51840/60000]\n",
      "loss: 0.074705 [52480/60000]\n",
      "loss: 0.117567 [53120/60000]\n",
      "loss: 0.095401 [53760/60000]\n",
      "loss: 0.246345 [54400/60000]\n",
      "loss: 0.117524 [55040/60000]\n",
      "loss: 0.266166 [55680/60000]\n",
      "loss: 0.081762 [56320/60000]\n",
      "loss: 0.041123 [56960/60000]\n",
      "loss: 0.119245 [57600/60000]\n",
      "loss: 0.220643 [58240/60000]\n",
      "loss: 0.233364 [58880/60000]\n",
      "loss: 0.089249 [59520/60000]\n",
      "Test Accuracy: 91.1%, Avg loss: 0.270698\n",
      "\n",
      "training complete\n"
     ]
    }
   ],
   "source": [
    "# Define Residual Block\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, num_channels, use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1, stride=strides)\n",
    "        self.conv2 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.LazyBatchNorm2d()\n",
    "        self.bn2 = nn.LazyBatchNorm2d()\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1, stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        return F.relu(Y + X)\n",
    "\n",
    "# Define ResNet\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, arch, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(self.b1())\n",
    "        for i, b in enumerate(arch):\n",
    "            self.net.add_module(f'b{i+2}', self.block(*b, first_block=(i==0)))\n",
    "        self.net.add_module('last', nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(num_classes)\n",
    "        ))\n",
    "\n",
    "    def b1(self):\n",
    "        return nn.Sequential(\n",
    "            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.LazyBatchNorm2d(), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "    def block(self, num_residuals, num_channels, first_block=False):\n",
    "        blk = []\n",
    "        for i in range(num_residuals):\n",
    "            if i == 0 and not first_block:\n",
    "                blk.append(Residual(num_channels, use_1x1conv=True, strides=2))\n",
    "            else:\n",
    "                blk.append(Residual(num_channels))\n",
    "        return nn.Sequential(*blk)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# Define ResNet18 Architecture\n",
    "\n",
    "class ResNet18(ResNet):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__(((2, 64), (2, 128), (2, 256), (2, 512)), num_classes)\n",
    "\n",
    "\n",
    "# Training and Testing Functions\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            current = batch * len(X)\n",
    "            print(f\"loss: {loss.item():>7f} [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Accuracy: {(100 * correct):>0.1f}%, Avg loss: {test_loss:>8f}\\n\")\n",
    "\n",
    "\n",
    "# Training Loop\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "model = ResNet18().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "epochs = 10\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n\" + \"-\"*30)\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "\n",
    "print(\"training complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afc45357-4b14-4701-9055-325d0b26c7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model \n",
    "\n",
    "EPOCH = epochs\n",
    "\n",
    "# define path\n",
    "\n",
    "PATH = \"assignment3_model.pt\"\n",
    "\n",
    "#save the model\n",
    "\n",
    "torch.save({\n",
    "    'epoch': EPOCH,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9c16d1-49d4-4c9b-857d-e9e3643bccb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "mode.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_stat_dict(checkpoint['optimizer_state_dict'])\n",
    "EPOCH = checkpoint['epoch']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
